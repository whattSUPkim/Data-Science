{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd39daab-1497-44ab-8596-975f32907c74",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 탐색적 데이터 분석(Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f941f-7100-4d62-a4cd-425643d4a703",
   "metadata": {},
   "source": [
    "## -  탐색적 데이터 분석 정의\n",
    "- 기존의 통계학이 정보의 추출에서 가설 검정 등에 치우쳐 자료가 가지고 있는 본연의 의미를 찾는데 어려움이 있어 이를 보완하고자 주어진 자료만 가지고도 충분한 정보를 찾을 수 있도록 여러가지 탐색적 자료 분석 방법을 개발(by 존 튜키)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8273cf9-33c0-4b02-ac35-d2db580a0789",
   "metadata": {},
   "source": [
    "## - 탐색적 데이터 분석의 필요성\n",
    "- 데이터 분포 및 값을 검토함으로써 데이터가 표현하는 현상을 더 잘 이해하고, 데이터에 대한 잠재적인 문제를 발견\n",
    "- 본격적인 분석에 들어가기에 앞서 데이터를 다시 수집하거나 추가로 수집하는 등의 결정을 내릴 수 있음\n",
    "- 데이터를 다양한 각도에서 살펴보는 과정을 통해 문제 정의 단계에서 미처 발생하지 못했을 다양한 패턴을 발견하고, 이를 바탕으로 기존의 가설을 수정하거나 새로운 가설을 세울 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1dfba-1a35-48fe-817e-4ac565616c75",
   "metadata": {},
   "source": [
    "## - 탐색적 데이터 분석 과정\n",
    "> 문제정의(질문, 가설) -> 분석 계획 -> 데이터 관찰 -> 재현 -> 요인 추가 / 가설 변경 -> 실험\n",
    "- 문제 정의 단계에서 세웠던 연구 질문과 가설을 바탕으로 분석 계획을 세우는 것\n",
    "    - 분석 계획에는 어떤 속성 및 속성 간의 관계를 집중적으로 관찰해야 할지, 이를 위한 최적의 방법은 무엇인지가 포함되어야 함\n",
    "- 분석의 목적과 변수가 무엇이 있는지를 확인하고, 개별 변수의 이름이나 설명을 가지는지 확인\n",
    "- 데이터를 전체적으로 살펴보기\n",
    "    - 데이터에 문제가 없는지\n",
    "    - head나 tail 확인\n",
    "    - 이상치, 결측치 등 탐색\n",
    "- 데이터의 개별 속성값을 관찰\n",
    "    - 각 속성 값이 예측한 범위와 분포를 갖는지 확인\n",
    "    - 만약 그렇지 않다면, 이유가 무엇인지를 확인\n",
    "- 속성 간의 관계에 초점을 맞춰, 개별 속성 관찰에서 찾아내지 못했던 패턴을 발견(상관관계, 시각화 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9acca8-42a3-4248-861c-48a5a8eb9ca1",
   "metadata": {},
   "source": [
    "## - 이상값 발견 기법\n",
    "- 개별 데이터 관찰\n",
    "- 통계값 활용: 요약 통계 지표(summary statistics)\n",
    "    - 5가지 숫자 요약(5-number summary)\n",
    "        - 최대값(maximum)\n",
    "        - 상위 사분위수 또는 제3사분위수(Q3): 중앙값 기준으로 상위 50% 중의 중앙값(상위 25%)\n",
    "        - 중앙값(median): 데이터의 가운데 **순위**에 해당하는 값 (!=mean)\n",
    "        - 하위 사분위수 또는 제1사분위수(Q1): 중앙값 기준으로 하위 50% 중의 중앙값(하위 25%)\n",
    "        - 최소값(miniumum)\n",
    "        > IQR(InterQuartile Range): Q1 ~ Q3을 묶은 박스\n",
    "    - 속성 간의 관계분석  \n",
    "      | 데이터 조합 | 요약통계 | 시각화 |\n",
    "      |----------|--------|-------|\n",
    "      | 질적자료 + 질적자료 | 교차 테이블 | 모자이크 플롯 |\n",
    "      | 양적자료 + 질적자료 | 질적자료별 통계 값 | 박스 플롯 |\n",
    "      | 양적자료 + 양적자료 | 상관계수 | 산점도 |\n",
    "- 시각화 활용: 확률 밀도 함수, 히스토그램, 점 플롯, 워드 클라우드, 시계열 차트, 지도 등\n",
    "- 머신러닝 기법 활용: 클러스터링 등\n",
    "- 통계 기반 탐지: Distribution-based, Depth-based\n",
    "- 편차 기반 방법: Sequential exception, OLAP data cube\n",
    "- 거리 기반 탐지: Index-based, Nested-loop, Cell-based, Local-outliers, Partition-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c9691-d1ae-4659-8b8a-a5825ddf025b",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 데이터 전처리, 품질(Data Preprocessing, Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa640ed9-04dc-45b8-98bb-ce36df3cb59b",
   "metadata": {},
   "source": [
    "## - 데이터 전처리\n",
    "- 데이터를 분석 및 처리에 적합한 형태로 만드는 과정을 총칭\n",
    "- 일반적으로 데이터는 비어있는 부분이 많거나 정합성이 맞지 않는 경우가 많음(품질이 낮은 데이터)\n",
    "- 데이터 전처리 단계\n",
    "    - 데이터 정제(Cleaning): 누락 데이터나 잡음, 모순된 데이터 등을 정합성이 맞도록 교정하는 작업(ex.누락값, 불일치, 오류의 수정)\n",
    "    - 데이터 통합(Integration): 여러 개의 데이터베이스, 데이터세트 또는 파일을 통합하는 작업\n",
    "    - 데이터 축소(Reduction): 샘플링, 차원축소, 특징 선택 및 추출을 통해 데이터 크기를 줄이는 작업(데이터의 완결성을 유지하면서 양을 줄이는 작업)\n",
    "    - 데이터 변환(Transformation): 데이터를 정규화, 이산화 또는 집계를 통해 변환하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338982d-56ca-4b13-bcc0-0200a0a7803a",
   "metadata": {},
   "source": [
    "## - 데이터 품질\n",
    "- 완벽한 데이터를 얻는 것은 불가능함.\n",
    "- 데이터 품질을 저해하는 주요 요인 2가지\n",
    "    - 측정 오류: 사람의 실수로 잘못된 단위로 기록을 하거나 측정 장비 자체의 한계 등 측정 과정에서 발생하는 오류\n",
    "    - 수집 과정 오류: 데이터의 손실 중복 등의 문제로 발생하는 오류\n",
    "- 실시계의 데이터 난제\n",
    "    - 너무 많은 데이터: 손상 및 잡음, 속성의 수가 많은 데이터, 숫자와 문자가 혼합된 데이터\n",
    "    - 너무 적은 데이터: 결여된 속성, 결측치가 있는 데이터\n",
    "    - 파손된 데이터: 양립할 수 없는 데이터 등\n",
    "- 잡음: 측정 과정에서 무작위로 발생하여 측정값의 에러를 발생시키는 것\n",
    "- 아티펙트: 어떠한 요인으로 인해 반복적으로 발생하는 왜곡이나 에러(ex. 카메라 렌즈에 얼룩이 묻어 있다면, 얼룩으로 인한 왜곡이 지속적으로 발생)\n",
    "- 정밀도(Precision): 동일한 대상을 반복적으로 측정했을 때 각 결과의 친밀성을 나타내는 것\n",
    "- 바이어스(Bias): 편향된 데이터\n",
    "- 이상치(Outlier): 대부분의 데이터와 다른 특성을 보이거나 특성 속성의 값이 다른 개체들과 달리 유별난 값을 가지는 데이터를 의미. 잡음과는 달리 그 자체가 중요한 분석의 목적이 될 수 있음\n",
    "- 결측치(Missing values)\n",
    "- 모순, 불일치: 동일한 개체에 대한 측정 데이터가 다르게 나타나는 경우(ex. 주소와 우편번호가 다를 경우)\n",
    "- 중복(Duplocate data): 데이터 중복은 언제든지 발생 가능하나, 문제는 중복된 데이터 사이에 속성의 차이나 값의 약간의 불일치가 발생할 수 있음(적합한 데이터 선택 등의 추가적인 작업 필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9811a48-080c-4737-9b71-fe84743eb439",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 데이터 정제(Data Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd952f61-f87d-4d20-88dc-cd55f6ee528f",
   "metadata": {},
   "source": [
    "## - 데이터 특성 파악 -> 데이터 모순점 발견 -> 데이터 수정 변환 (반복)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d86dba-3aac-4d47-a2c0-31377fef45fb",
   "metadata": {},
   "source": [
    "## - 데이터 특성 파악\n",
    "- 속성의 데이터 타입과 도메인\n",
    "- 속성 값의 분포 특성\n",
    "    - 대칭/비대칭 분포\n",
    "    - 실제 값의 주요 분포 범위\n",
    "    - 값의 표준편차\n",
    "- 속성 간의 의존성: 속성 A의 값이 같은 데이터의 속성 B값과 같다면, 속성 A와 속성 B간 함수적 종속성 존재(A->B)\n",
    "- 메타데이터\n",
    "    - 데이터에 대한 데이터\n",
    "    - 메타-메타데이터: 메타데이터가 어떻게 구성되고 관리되는지에 대한 정보 제공\n",
    "    - 메타데이터의 종류\n",
    "        - 설명 메타데이터: 데이터에 대한 속성, 값에 대한 설명 제공\n",
    "        - 구조 메타데이터: 데이터가 어떻게 만들어졌는지에 대한 구조적 정보 제공\n",
    "        - 관리 메타데이터: 데이터의 생성, 유지, 관리에 대한 정보 제공(작성자, 변경자, 권한, 감사, 계보 등)\n",
    "        - 보존 메타데이터: 데이터 보관 절차를 지원하는데 필요한 정보 제공(저장소, 버전, 개발환경 등)\n",
    "        - 사용 메타데이터: 데이터가 어떻게 사용되었는지에 대한 정보 제공\n",
    "        - 거래 메타데이터: 데이터가 어떻게 거래되었는지에 대한 정보 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60012016-51d3-4f86-ae51-c8c2021695cc",
   "metadata": {},
   "source": [
    "## - 데이터 모순점 발견\n",
    "- 잘못 설계된 데이터 입력 폼이 존재\n",
    "- 데이터 입력에서 사람의 실수로 발생\n",
    "- 만료된 데이터(바뀐 주소 등)\n",
    "- 데이터 표현의 모순\n",
    "- 일치하지 않는 코드의 사용\n",
    "- 데이터를 기록하는 계측 장치의 오류나 시스템 오류\n",
    "- 원래의 의도와 다른 목적으로 데이터를 부적절하게 사용\n",
    "- 데이터 통합 과정에서 주어진 속성이 다른 데이터베이스에서 다른 이름을 사용할 경우\n",
    "- 필드 오버로딩\n",
    "    - 데이터 분석가는 코드 사용의 불일치와 데이터 표현의 불일치를 주의해야함\n",
    "    - 필드 오버로딩은 개발자가 기존에 정의된 속성에서 사용하지 않은 일부를 새로운 속성의 정의로 사용할 때 발생\n",
    "    - 필드 오버로딩에 대한 검토\n",
    "        - 유일규칙: 주어진 속성의 값이 같은 속성의 다른 값들과는 달라야 함\n",
    "        - 일관규칙: 최소값과 최대값 사이에 결측치가 없어야 함\n",
    "        - 무 규칙: 공백, 물음표, 특수문자 등과 같이 데이터가 없음을 나타내는 다른 문자의 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1045f7-6c1c-4cb1-b0fc-54f8dcfb01ba",
   "metadata": {},
   "source": [
    "## - 데이터 수정 변환\n",
    "- 모순점이 발견된 데이터에 대해 수정 변환 필요\n",
    "- 데이터 수정 변환 시 오류 발생 가능성도 높고, 많은 시간이 필요(더 많은 모순이 생길 수 있음)\n",
    "- ex. 년도 속성에 20001이라는 값은 다른 날짜 값들이 일관된 형식(YYYY)로 변환될 때 \n",
    "- ex. 성별 속성에 'emale' 이라는 값은 다른 성별 값들이 일관된 형식(1:male, 2:female)으로 변환될 때\n",
    "- 결측값: 값이 존재하지 않고 비어있는 상태\n",
    "    - NA: 결측값\n",
    "    - NULL: 값이 없다\n",
    "    - 결측값 구분\n",
    "        - MCAR(Missing Completely At Random)\n",
    "            - 결측값이 관측된 데이터와 관측되지 않은 데이터와 독립적이며 완전 무작위로 발생\n",
    "            - 데이터 분석 시 편향되지 않아 결측값이 문제되지 않는 경우\n",
    "        - MAR(Missing At Random)\n",
    "            - 결측값이 다른 변수에 따라 조건부로 무적위 발생하는 경우\n",
    "            - 변수의 조건에 따른 결측값이 설명할 수 있는 경우\n",
    "            - 데이터 분석 시 편향이 발생할 수도 있음\n",
    "        - MNAR(Missing Not At Random)\n",
    "            - 무시할 수 없는 무응답 데이터(누락된 이유가 존재)\n",
    "            - 주도면밀한 추가 조사가 필요한 경우\n",
    "    - 결측값 처리 방법\n",
    "        - 결측값 데이터 개체 또는 속성의 제거(실제 많이 사용하진 않음)\n",
    "        - 수동으로 결측값 입력\n",
    "            - 데이터를 다시 조사 및 수집\n",
    "            - 매우 고비용의 소모적인 방법\n",
    "        - 전역상수(global constant)를 사용한 결측값 입력\n",
    "            - 예를 들어, 결측값을 0으로 입력\n",
    "            - 분석 결과를 왜곡할 수 있음\n",
    "        - 결측값의 무시\n",
    "            - 하나의 속성 값이 없더라도 유사성을 계산하는데 미치는 영향이 크지 않다면 무시\n",
    "            - ex. 개체들 사이의 유사성 계산에 있어 많은 수의 속성이 있는 경우 이 중 하나의 속성이 없다면 이를 제외하고 유사성을 계산할 수 있또록 알고리즘을 조정하는 것\n",
    "        - 결측값의 추정(일반적으로 많이 사용되는 방법)\n",
    "            - 속성의 평균값을 사용하여 결측값 추정(왜곡시킬 위험성 존재)\n",
    "            - 같은 클래스(분류)에 속하는 속성의 평균값 사용(몸무게 속성 중 남자 몸무게, 여자 몸무게)\n",
    "            - 가장 가능성이 높은 값으로 결측값 추정(회귀분석, 베이지안 기법, 의사결정트리 기법 등의 통계 또는 마이닝 기법을 활용한 결측값 예측. 그러나 복잡함)\n",
    "- 잡음(Noise data)\n",
    "    - 측정된 변수(속성)에서의 오류나 오차 값\n",
    "    - 오류나 오차 값에 의해 경향성 훼손 발생\n",
    "    - 데이터 평활화 기법\n",
    "        - 구간화(Binning)\n",
    "            - 정렬된 데이터 값들을 몇 개의 빈으로 분할하여 평활화하는 방법\n",
    "            - 이웃(주변 값)들을 참조하여 정렬된 데이터를 매끄럽게 함\n",
    "                - 평균값 평활화, 중앙값 평활화, 경계값 평활화\n",
    "        - 회귀\n",
    "            - 회귀함수를 이용한 데이터 평활화 기법\n",
    "            - 선형 회귀 분석\n",
    "            - 다중 회귀 분석(두 개 이상의 속성을 가지고 다른 속성을 예측)\n",
    "        - 군집화\n",
    "            - 유사한 값들끼리 그룹화하는 과정\n",
    "            - 이상값: 어떤 군집에도 속하지 않은 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549a2da-a24c-4142-8db8-7b6091a17eb7",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 데이터 통합(Data Integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5696a3-a7a2-4f0d-99d7-24b4625b1571",
   "metadata": {},
   "source": [
    "## - 개체 식별 문제(Entity Identification Problem)\n",
    "- 데이터 통합은 여러 데이터 저장소로부터 온 데이터의 합병\n",
    "- 데이터 원천은 데이터베이스 데이터 큐브, 플랫 파일 등 다양한 형태로 존재\n",
    "- 여러 데이터 원천들로부터 데이터를 통합시킬 때, 동일한 의미의 개체들이 서로 다르게 표현되어 있을 경우, 어떻게 일치시킬 수 있을까?\n",
    "    > 개체 식별 문제\n",
    "- 메타데이터의 활용이 중요\n",
    "    - 각 속성의 메타데이터는 이름과 의미, 데이터 타입, 허용되는 값의 범위, 무규칙(null rule) 등을 포함\n",
    "    - 스키마 통합 과정에서 오류를 피하기 위해 메타데이터 활용 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd660d9-fa55-4bbb-9149-10f140cdfc28",
   "metadata": {},
   "source": [
    "## - 유도 속성\n",
    "- 어떤 속성은 다른 속성이나 속성의 집함으로부터 유도 가능\n",
    "    - 총점 속성은 각 과목 점수 속성 집합의 유도 속성\n",
    "- 반 정규화(de-normalization)\n",
    "    - 정규화의 반대 개념으로 유도 속성의 정의\n",
    "    - 일반적으로 정규화는 데이터 중복을 제거하는 과정\n",
    "    - 반정규화는 성능 향상을 위해 중복을 허용하는 과정(데이터 불일치 문제가 발생할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6ebe7-9489-41e2-a2bf-239083036fae",
   "metadata": {},
   "source": [
    "## - 정규화되지 않은 테이블\n",
    "- 정규화되지 않은 테이블의 사용은 데이터 중복의 원인이 됨\n",
    "    - 삽입이상: 새 데이터를 삽입하기 위해 불필요한 데이터도 삽입하는 현상\n",
    "        - ex. 키에 해당하지 않는 경우, 기본키 칼럼은 NULL이 불가하므로, 삽입하려면 '수강 미정'과 같은 과목코드를 불필요하게 삽입해야함\n",
    "    - 갱신이상: 중복 튜플 중 일부만 변경하여 데이터가 불일치되는 모순의 문제\n",
    "        - ex. a학생이 유일한 '컴공'에서 '경영학과'로 전과하면 전체 학과 정보 변경이 필요(컴공이 없어짐)\n",
    "    - 삭제이상: 튜플 삭제 시 필요한 데이터까지 함꼐 삭제되는 데이터 손실 문제\n",
    "        - ex. a라는 학생이 유일하게 수강신청한 b과목을 수강취소 할 경우, a학생의 정보가 삭제되는 이상 발생\n",
    "- 일부러 정규화하지 않은 채 테이블을 사용하는 경우, 조인을 피함으로 성능을 향상시키지만, 이러한 테이블 구조는 데이터 중복으로 인한 데이터 일관성 저해 문제 발생\n",
    "- 함수적 종속성\n",
    "    - 부분함수적 종속: 속성집합 y가 속성집합 x 전체가 아닌 부분에도 함수적으로 종속\n",
    "    - 완전 함수적 종속: 속성집합 y가 속성집합 x 전체에 대해서 함수적으로 종속된 경우\n",
    "- 정규화\n",
    "    - 중복요소를 찾아 제거해 나가는 과정\n",
    "    - 제1정규화: 하나의 속성이 원자값(하나씩)을 갖도록 설계를 변경하는 과정\n",
    "    - 제2정규화: 주식별자가 아닌 속성들 중에서 주식별자 전체가 아닌 일부 속성에 종속된 속성을 찾아 제거하는 과정\n",
    "        - 제1정규형에 속하고, 기본키가 아닌 모든 속성이 기본키에 완전종속\n",
    "        - 삽입 이상, 삭제 이상이 발생할 수 있음\n",
    "    - 제3정규화: 주식별자가 아닌 속성들 중에서 종속관계에 있는 속성을 찾아 제거하는 과정\n",
    "        - 기본키가 아닌 모든 속성이 기본키에 이행적 함수 종속이 되지 않으면 제3정규형\n",
    "        - 이행적 함수 종속: x y z에 대해서 x->y이고, y->z이면, x->z가 성립(z가 x에 이행적으로 함수 종속)\n",
    "    - (상관분석)\n",
    "        - 두 속성간의 엄격한 함수적 종속 관계가 성립하지는 않지만, 상관분석을 통해 한 속성이 다른 속성을 얼마나 강하게 암시하는지를 데이터를 통해 측정\n",
    "        - 두 속성 간 상관도(상관계수)가 높다면 두 속성을 중복으로 보고 그 중 하나의 속성을 제거할 수 있음\n",
    "        - 상관관계가 인과관계를 직접 의미하는 것은 아님\n",
    "    - (카이제곱 검정)\n",
    "        - 범주형(이산형) 데이터인 경우, 속성 A와 B 사이의 상관관계는 피어슨의 카이제곱 검정에 의해 측정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08550379-5ad4-4357-9832-b1521ce13fdb",
   "metadata": {},
   "source": [
    "# 데이터 축소(Data Reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f32ab7-d791-46a9-8ed5-e37c8d0a6034",
   "metadata": {},
   "source": [
    "## - 데이터 축소\n",
    "- 방대한 양의 데이터를 대상으로 복잡하게 데이터를 분석하고 마이닝 기법을 적용한다면 매우 많은 시간이 소요되어 분석이 비현실적임 -> 데이터 축소 필요\n",
    "- 데이터 축소 전략\n",
    "    - 차원적 축소: 데이터 인코딩 스키마를 적용하여 압축되거나 축소된 표현 제공\n",
    "    - 수치적 축소: 모수적 모형이나 비모수적 모형을 사용한 데이터 대체\n",
    "        - 모수적 모형: 모수의 특성을 활용하는 모형으로 모집단이 정규분포를 띈다는 가정하에 표본 통계량으로 모집단 통계량을 추정\n",
    "        - 비모수적 모형: 모수의 특성을 활용하지 않는 모형으로 군집화, 표본 추출, 히스토그램 등이 대표적인 예\n",
    "- 축소된 데이터 집합에 대한 데이터 분석 결과는 원본 데이터 집합에 대한 결과와 거의 동일한 결과를 산출해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a9479-b5c2-4e24-89a3-c178db5b7308",
   "metadata": {},
   "source": [
    "## - 데이터 큐브\n",
    "- 데이터 큐브는 데이터웨어하우스에서 나오는 용어\n",
    "- 다차원 집계 정보를 의미\n",
    "- 데이터 큐브를 구성하는 각 셀은 다차원 공간에서 데이터 포인트에 일치하는 집계된 데이터를 가짐\n",
    "- 원천 데이터를 여러 관점에서 추상화시켜 데이터 축소를 구현\n",
    "- 데이터 큐브는 사전 계산(precomputed)이 되고, 요약된 데이터에 신속히 접근할 수 있도록 하며, 다양한 데이터 분석 처리가 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845a0b1-7adc-442b-9bf6-7e63b74d8612",
   "metadata": {},
   "source": [
    "## - 속성 부분집합 선택\n",
    "- 속성 부분집합 선택은 연관성이 낮거나 중복되는 데이터 속성을 제거하여 데이터 집합의 크기를 줄이는 기법을 의미\n",
    "- 속성 중에서 데이터 분석에 영향을 미치지 않거나 타 속성과 중복적 성격을 가지는 것도 많이 존재\n",
    "- 목표는 전체 속성에 가장 가까운 데이터 범주의 확률 분포와 최소의 속성 집합을 찾는 것\n",
    "- 최소의 속성 집합을 찾는 법\n",
    "    - 경험적 기법(휴리스틱)\n",
    "        - 단계적 전진 선택법: 속성의 공집합으로 시작해서 최적의 속성들을 하나씩 추가하는 방법\n",
    "        - 단계적 후진 제거법: 속성의 전체집합으로 시작해서 최악의 속성들을 하나씩 제거하는 방법\n",
    "        - 전진 선택법과 후진 제거법의 결합: 각 단계마다 최선의 속성을 선택하고 최악의 속성을 제거하는 방법\n",
    "        - 의사결정트리 귀납법: 의사결정트리는 데이터마이닝 기법 중 분류를 위해 고안되었고, 흐름도와 유사한 구조를 가짐\n",
    "            - 분기가 거듭되면 데이터의 개수는 줄어듦(데이터가 분할됨)\n",
    "            - 분류와 회귀 모두 사용 가능\n",
    "            - 범주형 또는 연속형 데이터에 대해서 예측 가능\n",
    "            - (Root node: 트리의 초기 지점, Terminal node: 수가 분리된 집합의 수(터미널 노드를 합치면 전체 데이터의 수와 동일))\n",
    "            - 의사결정트리는 분할(구분) 한 뒤 각 영역의 순도가 증가/불확실성(엔트로피)이 최대한 감소하는 방향으로 학습되어야 함\n",
    "                - 불순도/불확실성이 감소하면(순도가 증가하면) 정보이론에서는 정보획득이라 함\n",
    "                - 엔트로피(불확실성)\n",
    "                    - A영역에 속한 모든 레코드가 동일한 범주에 속한 경우 엔트로피 0(순도 최고)\n",
    "                    - 반대로 범주가 둘 뿐이고 해당 개체의 수가 반반씩 섞여 있을 경우 엔트로피 1(순도 최소)\n",
    "                - 지니 계수\n",
    "                - 오분류오차(misclassification error): 불순도 측정이 가능하지만 미분이 불가하여 자주 쓰이지 않음\n",
    "            - 의사결정트리 모델 학습\n",
    "                - 재귀적 분기(recursive partitioning): 입력 변수 영역을 두 개로 구분\n",
    "                    - 가능한 모든 분기점에 대해 엔트로피/지니 계수를 구해 분기 전과 비교해 정보 획득(정보 획득이 가장 큰 변수와 그 지점으로 첫 번째 분기를 선택, 이후 반복)\n",
    "                    - ex. 나눌 수 있는 모든 분기점을 row번호로 나열한다면 (1 | 2:24), (1:2 | 3:24), ... (1:23 | 24) 이다.\n",
    "                - 가지치기(pruning): 너무 자세하게 구분된 영역을 통합\n",
    "                    - full tree: 모든 terminal node의 순도가 100%인 상태\n",
    "                    - 분기가 너무 많으면 학습 데이터에 과적합 될 수 있음 -> 분기 수가 증가할 때, 처음에는 새로운 데이터에 대한 오분류율이 감소하나 일정 수준 이상이 되면 우분류율이 증가하는 현상이 발생\n",
    "                    - 오분류율이 증가하는 시점에 가지치기 수행(merge)\n",
    "                    - 가지치기 비용함수: CC(T)값 (오류가 적으면서 terminal node 수가 적은 단순한 모델일수록 작은 값이 나옴) 활용\n",
    "                    \n",
    "    - 소모적 탐색법: 2**n개의 가능한 속성 조합을 모두 탑색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207d7a4-f85f-45cf-a349-20e1ccd1176b",
   "metadata": {},
   "source": [
    "## - 차원 축소\n",
    "- 원천 데이터의 축소판을 얻기 위한 데이터 부호화 또는 데이터 변환의 적용\n",
    "    - 원천 데이터 정보의 손실 없이 압축된다면 무손실(loseless)\n",
    "    - 원천 데이터의 근사치만으로 축소된다면 손실(lossy)\n",
    "- 일반적으로 많이 사용되며 효과적인 손실 차원 축소 방법\n",
    "    1. 웨이블릿 변환(wavelet transform) - 고차원 데이터에 적합\n",
    "        - 이산 웨이블릿 변환(discrete wavelet transform, DWT): 데이터 벡터 X를 다른 수치적 백터 X`으로 변환(X와 X`의 길이는 동일. 변환 데이터가 압축되어 보임)\n",
    "        - ex. 사용자가 정한 어떤 임계값보다 큰 모든 웨이블릿 계수들만 값을 유지하고 나머지 계수들을 0으로 간주\n",
    "        - 데이터 주요 특징들은 보존하면서도 잡음을 제거하는 역할을 하기도 하므로 데이터 정제를 위해서도 효과적임\n",
    "    2. 주성분 분석(principal components analysis) - 희소 데이터 취급에 유리\n",
    "        - 주성분 분석은 n개의 속성을 가진 튜플(n차원의 데이터 벡터)에 대해 데이터를 효현하는데 최적으로 사용될 수 있는 n차원 직교벡터(orthogonal vector)들에 대한 k를 찾음(k <= n)  ->  감소된 차원의 공간을 갖는 데이터 공간 생성(차원 축소)\n",
    "        - 주성분 분석 절차\n",
    "            1. 입력 데이터를 표준화하여 같은 범위에 속하게 함(큰 범위를 갖는 속성이 작은 범위를 갖는 속성들을 압도하지 않도록)\n",
    "            2. 표준화된 입력 데이터를 위한 기저(base)를 제공하는 직교 벡터들을 계산 - 이들을 주성분이라고 하며, 입력 데이터는 주성분의 선형 조합\n",
    "            <img src=\"https://t1.daumcdn.net/cfile/tistory/25388D40527C43DB0B?download\" width=\"300\" height=\"300\"/>\n",
    "            3. 주성분은 중요도의 내림차순으로 정렬. 주성분은 데이터에 대한 새로운 축의 집압으로서의 역할(즉, 정렬된 첫 번째 축은 가장 큰 분산을 보여주며, 두 번째 축은 그 다음으로 높은 분산을 보여주는 식)\n",
    "            4. 내림차순 정렬이 되어있어 약한 주성분을 제거함으로써 데이터 크기 감소\n",
    "- 그 외 방법\n",
    "    1. 회귀 모형\n",
    "        - 주어진 데이터의 근사치를 구하는데 사용\n",
    "    2. 로그-선형 모형\n",
    "        - n개의 속성으로 표현되는 n차원에서 주어진 n개의 튜플 집합을 n차원 공간의 한 점으로 생각하는 이산 다차원 확률분포의 근사치를 구함\n",
    "        - 회귀 방법이 고차원 데이터에 적용될 경우 계산 비용이 기하급수적으로 늘어남(로그-선형 모형은 10차원 정도까지는 우수한 확장 가능성을 보여줌)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac08e45-841b-42ab-b890-774f799a9ae1",
   "metadata": {},
   "source": [
    "## - 수량 축소 방법\n",
    "- 표본 추출\n",
    "    - 큰 데이터 집합을 많은 수의 임의 데이터 샘플(부분집합)로 표현 가능\n",
    "    - 표본 추출 방법\n",
    "        - 비복원 단순 무작위 표본: D로부터 N개의 튜플 중에서 임의의 s개를 취하는 방법으로서 모든 튜플들의 표본으로 추출될 확률은 같음\n",
    "        - 복원 단순 무작위 표본: 각 튜플이 D로부터 추출될 때마다 기록된 후 다시 제자리로 replace 됨(반복)\n",
    "        - 집략표본(Cluster sample): D에 있는 튜플들이 M개의 상호 배반적 군집으로 묶여 있는 가운데 s개의 군집을 단순 무작위로 추출(집단 내에서는 이질적이지만 집단 간 차이는 동질적)\n",
    "        - 층화표본(Stratified sample): D가 층(strata)이라 불리는 상호배반적 부분들로 분할되어 있따면, 각 층에서 단순 무작위로 추출(집단 내에서는 동질적이지만 집단 간 차이가 이질적)\n",
    "- 히스토그램\n",
    "    - 구간화를 사용하여 데이터 분포의 근사치를 구하는 데이터 축소의 전형적 형태\n",
    "    - 속성 A의 데이터를 버킷(bucket) 혹은 빈(bin)이라 불리는 분리 집합으로 나눔\n",
    "    - 히스토그램은 희소 데이터나 밀집 데이터 모두에 효과적. 비대칭적 데이터와 균일한 데이터에도 모두 효과적\n",
    "    - 단일 속성에 대한 히스토그램은 다중 속성에 대한 것으로 확장 가능\n",
    "    - 다차원 히스토그램에서는 속성 간 의존성 포착 가능\n",
    "    - 일반적으로 5개까지의 속성을 가진 데이터의 근사치를 구하는 데 효과적이라고 알려짐\n",
    "    - 버킷 결정 및 속성 값 분할 방법\n",
    "        - 동등 폭: 각 버킷의 범위는 균일\n",
    "        - 동등 빈도: 각 버킷의 빈도가 일정(각 버킷이 같은 수의 표본을 포함)\n",
    "        - v-최적(v-optimal): 최소 분산을 갖는 히스토그램을 의미. 히스토그램 분산은 각 버킷이 나타내는 데이터의 값들의 가중합이며, 버킷 가중치는 버킷에 있는 값들의 개수와 동일\n",
    "        - 최대 차이(Max-Diff): 인접한 값들의 각 쌍 사이의 차이를 고려, 사용자 정의 버킷의 수 β에 대하여, β-1개의 **최대 차이를 갖는 쌍**들에 대한 각 쌍 사이에 버킷 경계가 정해짐\n",
    "- 군집화\n",
    "    - 데이터 튜플을 객체로 간주하고, 각 객체들을 군집이라는 그룹으로 나눕\n",
    "    - 집단 내에서는 유사하면서도 다른 군집 내 객체들과는 유사하지 않도록 군집화\n",
    "    - 유사성은 공간 내에서 객체들이 어떻게 가까운지의 관점에 따라 거리 함수에 기반하여 정의\n",
    "    - 클러스터의 품질은 지름의 표현으로 나타내고(지름이 짧을수록 굿), 지름은 클러스터의 두 객체 간 최대거리로 표현(클러스터 간 중심 거리로 대체 가능, 길수록 굿)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077713d8-a3c8-442a-a38d-ffb87ae95c48",
   "metadata": {},
   "source": [
    "# 데이터 변환(Data Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c058b0-53c7-44a4-8bc1-4b6a5b9880eb",
   "metadata": {},
   "source": [
    "## - 데이터 변환\n",
    "- 데이터 변환은 데이터 분석에 적절한 형태로 데이터를 바꾸는 전처리 작업을 의미\n",
    "- 데이터 변환 방법\n",
    "    - 평활(Smoothing): 데이터로부터 잡음을 제거(구간화, 회귀, 군집화 등)\n",
    "    - 집계(Aggregation): 그룹화 연산을 데이터에 적용(일일 판매 데이터를 월별 또는 연도별로 그룹화하는 기법으로 주로 데이터 큐브를 구축하는 데 사용)\n",
    "    - 속성 구성(Attribute Construction): 주어진 속성 집합으로부터 새로운 속성을 구성하는 기법\n",
    "    - 정규화(Normalization): 데이터를 정해진 구간 내에 존재하도록 변환시키는 기법\n",
    "        - 정규화는 속성값으로 -1.0 ~ 1.0 등과 같이 정해진 구간 내에 들도록 하는 기법\n",
    "        - 정규화 방법\n",
    "            - 최소-최대 정규화(원본 데이터 값들 간의 관계를 보존)\n",
    "            - Z-score 정규화(최소값과 최대값이 알려져 있지 않거나, 최소-최대 정규화에 큰 영향을 주는 이상치가 존재할 때 유용)\n",
    "            - 소수 척도화(소수점을 이동해서 정규화)\n",
    "    - 이산화(Discretization): 수치형 속성의 원시값(raw-value)을 구간 라벨로 대체하거나 개념적인 라벨로 대체하는 기법\n",
    "        - 구간화(binning)\n",
    "        - 히스토그램\n",
    "        - 엔트로피 기반 이산화\n",
    "            - Top-down 방식의 이산화 기법\n",
    "            - 수치형 속성 A의 값들을 분할하기 위해 분할점으로 최소 엔트로피를 갖는 A의 값을 선택(계층적 이산화를 달성)\n",
    "        - 카이제곱 결합\n",
    "            - 대부분의 이산화 기법들이 Top-down 분할 방식을 적용하는 것과는 달리 카이제곱 기법은 가장 가까운 이웃 구간을 찾아내고 이들을 결합하여 더 큰 구간을 형성시키는 bottom-up 방식을 적용\n",
    "            - 카이제곱 결합 기법의 핵심 개념은 정교한 이산화를 위해 클래스 상대도수가 구간에 매우 일관성이 있어야 함\n",
    "            - 만약 두 개의 이웃 구간들이 매우 유사한 분포를 가진다면 그 구간들은 결합되고, 그렇지 않으면 분리된 상태로 둠\n",
    "            - 카이제곱 통계량은 주어진 속성에 대한 '두 개의 이웃 구간들의 독립이다'라는 가설 검정\n",
    "        - 군집 분석\n",
    "        - 직관적 불할에 의한 이산화\n",
    "    - 개념 계층(Conceptual Hierarchy): 도로명과 같은 속성을 시나 국가와 같은 상위 레벨 개념으로 일반화시키는 기법(많은 명목형 속성들이 암시적인 개념 계층을 가지고 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93697dd-2226-4e7e-a3a1-36525dc5e4e1",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc2454-a170-4906-bb1e-1cec8d959e31",
   "metadata": {},
   "source": [
    "## - Feature Engineering 정의\n",
    "- 머신러닝 알고리즘을 작동하기 위해 데이터에 대한 도메인 지식을 활용하여 특징을 만들어내는 과정\n",
    "- 머신러닝 모델을 위한 데이터 테이블의 컬럼(특징)을 생성하거나 선택하는 작업을 의미\n",
    "- Feature Engineering은 모델 성능에 미치는 영향이 크기 때문에 머신러닝 응용에 있어서 매우 중요한 단계이며, 전문성과 비용이 많이 드는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab967f2d-f9b7-48fe-8437-d17d7633c95a",
   "metadata": {},
   "source": [
    "## - 특징 선택(Feature seletion)\n",
    "- 분류 모델 중 의사결정트리 같은 경우는 트리의 상단에 있을수록 중요도가 높으므로 이를 반영하여 특징 별로 중요도를 매길 수 있음\n",
    "- 회귀 모델의 경우 forward selection과 backward elimination 같은 알고리즘을 통해 특징을 선택\n",
    "    - 필요한 부분을 선택 / 불필요한 부분을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018568e0-3d39-488c-9612-ee7e984d0695",
   "metadata": {},
   "source": [
    "## - 차원 감소\n",
    "- 차원 감소는 특징 추출이라는 말로도 불림\n",
    "- 차원축소는 단순히 데이터의 압축이나 잡음을 제거하는 것이 아님\n",
    "- 중요한 의의는 관측 데이터를 잘 설명할 수 있는 잠재공간(latent space)을 찾는 것\n",
    "- 가장 대표적인 알고리즘: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32324e0e-806b-4a16-9080-22450f07aa9e",
   "metadata": {},
   "source": [
    "## - 특징 생성(Feature Engineering)\n",
    "- 특징 구축이라고도 하며, 이 방법을 흔히 feature engineering이라고 말함\n",
    "- 초기에 주어진 데이터로부터 모델링 성능을 높이는 새로운 특성을 만드는 과정\n",
    "- 데이터에 대한 도메인 전문성을 바탕으로 데이터를 합치거나 쪼개는 등의 작업을 거쳐 새로운 feature를 만들게 됨\n",
    "    - ex. 시간 데이터를 AM/PM으로 나누는 것 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f70cd-f6b4-40d5-b48d-b9755eafae9d",
   "metadata": {},
   "source": [
    "## - Feature Engineering 처리(과정)\n",
    "1. 브레인스토밍 또는 특징 테스트\n",
    "2. 생성할 특징 결정\n",
    "3. 특징 생성\n",
    "4. 모델에서 특징이 어떻게 작동하는지 확인\n",
    "5. 필요한 경우 특징 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb8422-9fe1-4573-8442-1f19287963b1",
   "metadata": {},
   "source": [
    "## - Feature Engineering 방법 분류\n",
    "- 지표 변수\n",
    "    - 지표가 되는 변수를 만드는 것\n",
    "    - ex. 나이 feature로부터 21세 이상일 경우 성인으로 구분하는 feature 생성\n",
    "- 중복 특징\n",
    "    - 두 개의 특징을 결합하여 새로운 특징을 만드는 방법\n",
    "    - ex. 클릭 수와 접속 수를 결합하여 클릭 당 방문자수와 같은 feature 생성\n",
    "- 대표 특징\n",
    "    - 특징들로부터 대표성을 갖는 새로운 특징을 만드는 작업\n",
    "    - ex. 미국의 12학년 제도로 표시되는 데이터가 있을 때, 이를 기반으로 초/중/고교와 같이 대표성을 가지는 특징을 만들 수 있음\n",
    "- 외부 데이터\n",
    "    - 모델 성능을 높이기 위해 다른 데이터를 활용\n",
    "- 에러 분석\n",
    "    - 모델을 통해 나온 결과를 바탕으로 특징을 만드는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da0b6e-56e9-44f7-971e-783bc30de278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
